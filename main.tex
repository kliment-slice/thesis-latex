%==
%	DOCUMENT DEFINITION
%==

\documentclass[pdftex,11pt,titlepage,twoside,openright]{report}	


% for suporting multi platform
\usepackage[utf8x]{inputenc} 	
\usepackage{amsfonts}
% natbib is what you want for bibliography
\usepackage[square,authoryear]{natbib}


%--
%	DECLARATIONS
%--

\input{./includes/declarations.tex}


%--
%	INDEX AND GLOSSARY
%--


\usepackage[style=long,nonumberlist,toc,xindy,acronym,nomain]{glossaries} % nomain, if you define glossaries in a file, and you use \include{INP-00-glossary}
%\loadglsentries[main]{glossary}
% or using \input:
\include{glossary}
\makeglossaries
\usepackage{makeidx}
\makeindex



%==
%	BEGIN DOCUMENT
%==


\begin{document}

% roman numbering 
\setcounter{page}{1}
\pagenumbering{roman}

% print title
\input{./includes/title.tex}

%pagestyle after title
\pagestyle{fancy}

% table of contents,list of figures and list of tables
\setcounter{tocdepth}{1}
% \tableofcontents
% \listoffigures
% \listoftables
{\tableofcontents \let\cleardoublepage\clearpage \listoffigures 
\let\cleardoublepage\clearpage \listoftables \let\cleardoublepage\clearpage}

%\cleardoublepage

% ABSTRACT EXECUTIVE SUMMARY

\begin{abstract}
    This work investigates a novel application of a Vision Transformer (ViT) as a 
    quality assessment reference metric for generated images after neural image compression. 
    The Vision Transformer is a revolutionary implementation of the Transformer attention 
    mechanism (typically used in language models) to object detection in digital images. 
    The ViT architecture is designed to output a classification probability distribution against
    a set of training labels. 
    Thus, it is a suitable candidate for a new method for quantitative assessment of generated 
    images based on object-level deviations from the original pre-compression image.
    The metric will be referred to as a ViT-Score.
    This approach complements other comparative measurement techniques based on per-pixel 
    discrepancies (Mean Squared Error, MSE) or structural comparison (Structural Similarity Index, SSIM). 
    This study proposes an original end-to-end deep learning framework for neural image compression, 
    latent vector representation, regeneration, and image quality analysis using 
    state-of-the-art model architectures. Neural image compression and generation 
    is achieved using a Generative Adversarial Network (GAN). 
    Results from this work demonstrate that a ViT-Score from a Vision Transformer is capable of assessing 
    the quality of a neurally compressed image. 
    Moreover, this methodology provides valuable insights when measuring image quality.
    It can be used in addition to established perceived quality metrics for compressed and generated 
    images such as SSIM and Frechet Inception Distance (FID). 
    
    \ThinHRule
\end{abstract}

\pagestyle{fancy}

\setcounter{page}{1}
\pagenumbering{arabic}


%==
%	CHAPTER 1
%==

\chapter{Introduction to Vision Transformers (ViT)}


% Chapter overview 
This chapter presents the reader with an introduction to Vision Transformers (ViT).

It covers the motivation as to why ViT, or a future development inspired by it,
will have a profound impact on the future of image compression, analysis, and generation.
This chapter presents evidence that a Transformer, or perhaps an evolved deep learning model with a
similar architecture (i.e. generalizable and highly overparameterized) can be superior in
compressing and evaluating the latent feature space of a digital image compared to present-day
technologies.

This section then reviews the brief history of Transformer usage in deep learning.
These generalized architectures are now dominating state-of-the-art language models,
as they are extremely efficient in packing information within a one dimensional vector.

This introduction will then proceed to describe the principles of operation of a ViT. 
Then, proceed with a mathematical formulation. Finally, it will cover currently 
available implementations in the form of pre-trained models and conclude with an 
explanation on the computational and financial constraints of training such demanding 
architectures.

\ThinHRule

\newpage
\input{./includes/chapter1.tex}


%==
%	CHAPTER 2
%==
% \cleardoublepage
\chapter{Background Review: Transformers and Neural Image Compression}


This chapter serves to present the audience with a literature review of seminal academic
publications and relevant background information relating Transformers to image 
compression and generation.

Necessary formulations of Generative Adversarial Networks (GANs),
Image Compression, and Image Quality Assessment (IQA)
are also provided to aid the reader's understanding of this thesis project.

Among much of the domain knowledge available, the reader would find interest in the 
takeaways offered from:

\begin{itemize}
	\item "An Image is Worth 16x16 Words" \citep{dosovitskiy2020vit}
	\item "Towards End-to-End Image Compression and Analysis with Transformers" 

    \citep{Bai2022AAAI}
    \item Image Generation with GANs
	\item "First Principles of Deep Learning and Compression" \citep{Principles}
	\item Review of commonly used Image Quality Assessment (IQA) metrics
    
    \citep{Metrics}
\end{itemize}

\ThinHRule

\newpage
\input{./includes/chapter2.tex}

%==
%	CHAPTER 3: Core of Thesis
%==
% \cleardoublepage
\chapter{ViT-based Assessment of Neural Image Compression}

Now that we explained relevant transformer components, we can see how it applies to 
2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google 
team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to each other 
pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset 
of the image called image patches. Then, unroll each image patch into a sequence (256x1), 
and index it with a positional embedding in a table. All of that is then fed into a 
standard Transformer, like from Attention is all you need. Finally, a feed forward 
classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\ThinHRule

\newpage
\input{./includes/chapter3.tex}

%==
%	CHAPTER 4
%==
% \cleardoublepage
\chapter{Discussion}

Increase in relevant recent publications (13 alone in 2022 thus far) supports the vision of tying 
vision transformers to image compression.

\ThinHRule

\newpage
\input{./includes/chapter4.tex}

%==
%	CHAPTER 5
%==
% \cleardoublepage
\chapter{Summary}

This chapter serves to conclude this thesis. 

It provides a summary 
of original contributions made by the author while studying and experimenting 
with Vision Transformers (ViT) and Neural Image Compression, as well as the broader 
scientific domains of Machine and Deep Learning and Digital Image Processing. 

A summary of key takeaways is provided for the audience.

The author concludes by acknowledging key contributors to the project and serves closing
remarks.


\ThinHRule

\newpage
\input{./includes/chapter5.tex}

%==
%	BIBLIOGRAPHY
%==

% \cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Bibliography} % add entry to table of contents
\pagestyle{plain}


%{\textbf{\LARGE{Bibliography}}}\\	%headline
%\nocitep{*} % Show all Bib-entries (DEBUG)

\bibliographystyle{abbrvnat}
% \bibliography{bib/algorithm.bib,bib/resDes.bib} % for a better structure you can split your bib items into seperate files
\bibliography{bib/resDes.bib}

% \cleardoublepage
\newpage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Appendix} % add entry to table of contents
\pagestyle{plain}
{\textbf{\LARGE{APPENDIX}}}\\	%headline
\input{./includes/appendix.tex}

\end{document}