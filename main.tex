%==
%	DOCUMENT DEFINITION
%==

\documentclass[pdftex,11pt,titlepage,twoside,openright]{report}	


% for suporting multi platform
\usepackage[utf8x]{inputenc} 	
\usepackage{amsfonts}
% natbib is what you want for bibliography
\usepackage[square,authoryear]{natbib}


%--
%	DECLARATIONS
%--

\input{./includes/declarations.tex}


%--
%	INDEX AND GLOSSARY
%--


\usepackage[style=long,nonumberlist,toc,xindy,acronym,nomain]{glossaries} % nomain, if you define glossaries in a file, and you use \include{INP-00-glossary}
%\loadglsentries[main]{glossary}
% or using \input:
\include{glossary}
\makeglossaries
\usepackage{makeidx}
\makeindex



%==
%	BEGIN DOCUMENT
%==


\begin{document}

% roman numbering 
\setcounter{page}{1}
\pagenumbering{roman}

% print title
\input{./includes/title.tex}

%pagestyle after title
\pagestyle{fancy}

% table of contents,list of figures and list of tables
\setcounter{tocdepth}{1}
% \tableofcontents
% \listoffigures
% \listoftables
{\tableofcontents \let\cleardoublepage\clearpage \listoffigures 
\let\cleardoublepage\clearpage \listoftables \let\cleardoublepage\clearpage}

%\cleardoublepage

% ABSTRACT EXECUTIVE SUMMARY

\begin{abstract}
    This work investigates a novel application of a Vision Transformer (ViT) as a 
    quality assessment reference metric for generated images after neural image compression. 
    The Vision Transformer is a revolutionary implementation of the Transformer attention 
    mechanism (typically used in language models) to object detection in digital images. 
    The ViT architecture is designed to output a classification probability distribution against
    a set of training labels. 
    Thus, it is a suitable candidate for a new method for quantitative assessment of generated 
    image quality based on object-level deviations from the original pre-compression image.
    The metric is referred to as a ViT-Score.
    This approach complements other comparative measurement techniques based on per-pixel 
    discrepancies (Mean Squared Error, MSE) or structural comparison (Structural Similarity Index, SSIM). 
    This study proposes an original end-to-end deep learning framework for neural image compression, 
    latent vector representation, reconstruction, and image quality analysis using 
    state-of-the-art model architectures. Neural image compression and reconstruction 
    is achieved using a Generative Adversarial Network (GAN). 
    Results from this work demonstrate that a ViT-Score is capable of assessing 
    the quality of a neurally compressed image. 
    Moreover, this methodology provides valuable insights when measuring GAN output quality and
    can be used in addition to other relevant perceived quality metrics such as SSIM or Frechet Inception Distance (FID). 

    \ThinHRule
\end{abstract}

\pagestyle{fancy}

\setcounter{page}{1}
\pagenumbering{arabic}


%==
%	CHAPTER 1
%==

\chapter{Introduction to Vision Transformers (ViT)}


% Chapter overview 
This chapter presents the reader with an introduction to Vision Transformers (ViT).

It covers the motivation as to why ViT, or a future development inspired by it,
will have a profound impact on the future of image compression, analysis, and generation.
This chapter presents evidence that a Transformer, or perhaps an evolved deep learning model with a
similar architecture (i.e. generalizable and highly overparameterized) can be superior in
compressing and evaluating the latent feature space of a digital image compared to present-day
technologies.

This section summarizes the brief history of Transformer usage in deep learning.
These generalized architectures are dominating state-of-the-art language models,
as they are extremely efficient in packing relevant information within a one dimensional vector.

This introduction then proceeds to describe the principles of operation of a ViT, 
followed by its mathematical formulation. Finally, it covers currently 
available implementations in the form of pre-trained models and conclude with an 
explanation on the computational and financial constraints of training such demanding 
architectures.

\ThinHRule

\newpage
\input{./includes/chapter1.tex}


%==
%	CHAPTER 2
%==
% \cleardoublepage
\chapter{Background Review: Transformers and Neural Image Compression}


This chapter serves to present the audience with a literature review of seminal academic
publications and relevant background information relating Transformers to image 
compression and generation.

Necessary formulations of Generative Adversarial Networks (GANs),
Image Compression, and Image Quality Assessment (IQA)
are also provided to aid the reader's understanding of this thesis project.

Among much of the domain knowledge available, the reader would find interest in the 
takeaways offered from:

\begin{itemize}
	\item "An Image is Worth 16x16 Words" \citep{dosovitskiy2020vit}
	\item "Towards End-to-End Image Compression and Analysis with Transformers" 

    \citep{Bai2022AAAI}
    \item Overview of Image Generation with GANs
	\item "First Principles of Deep Learning and Compression" \citep{Principles}
	\item Image Quality Assessment (IQA) metrics
    
    \citep{Metrics}
\end{itemize}

\ThinHRule

\newpage
\input{./includes/chapter2.tex}

%==
%	CHAPTER 3: Core of Thesis
%==
% \cleardoublepage
\let\cleardoublepage\clearpage

\chapter{ViT-based Assessment of Neural Image Compression}


This chapter presents the core contributions from this thesis. Four input images
are used to demonstrate the modest ability of a GAN to compress and reconstruct from 
latent space. 
The section explains the architecture of choice and presents the output from the GAN.
The audience is presented with visualizations of the training process, latent space vector,
and resulting Compression Ratio (CR) values.
The reader is free to visually inspect the output.
The novel ViT-Score and its mathematical formulation is presented.
The output post-compression images are also evaluated using the following image quality metrics: 
SSIM, MSE, PSNR, FID, BRISQUE. 

Finally, a summary table presents all results from this chapter for quick reference. 

\ThinHRule

\newpage
\input{./includes/chapter3.tex}


%==
%	CHAPTER 4
%==
% \cleardoublepage
\chapter{Discussion}

This chapter critically discusses the results from Chapter 3 in greater detail.
The reader is presented with a thorough evaluation of the results, potential improvements, 
challenges, and possible optimization to the architecture.
Based on the results shown, the reader is invited to a thought-provoking discussion of the
present and future of image-based Transformers.
Finally, the audience is engaged in a speculation on the cost of training a successful image compressing Transformer.

\ThinHRule

\newpage
\input{./includes/chapter4.tex}

%==
%	CHAPTER 5
%==
% \cleardoublepage
\chapter{Summary}

This chapter serves to conclude this thesis. 

It provides a summary 
of original contributions made by the author while studying and experimenting 
with Vision Transformers (ViT) and Neural Image Compression, as well as the broader 
scientific domains of Machine and Deep Learning and Digital Image Processing. 

A summary of key takeaways is provided for the audience.

The author concludes by acknowledging key contributors to the project and serves closing
remarks.


\ThinHRule

\newpage
\input{./includes/chapter5.tex}

%==
%	BIBLIOGRAPHY
%==

% \cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Bibliography} % add entry to table of contents
\pagestyle{plain}


%{\textbf{\LARGE{Bibliography}}}\\	%headline
%\nocitep{*} % Show all Bib-entries (DEBUG)

\bibliographystyle{abbrvnat}
% \bibliography{bib/algorithm.bib,bib/resDes.bib} % for a better structure you can split your bib items into seperate files
\bibliography{bib/resDes.bib}

% \cleardoublepage
\newpage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Appendix} % add entry to table of contents
\pagestyle{plain}
{\textbf{\LARGE{APPENDIX}}}\\	%headline
\input{./includes/appendix.tex}

\end{document}