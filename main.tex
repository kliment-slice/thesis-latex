%==
%	DOCUMENT DEFINITION
%==

\documentclass[pdftex,11pt,titlepage,twoside,openright]{report}	


% for suporting multi platform
\usepackage[utf8x]{inputenc} 	

% natbib is what you want for bibliography
\usepackage[square,authoryear]{natbib}


%--
%	DECLARATIONS
%--

\input{./includes/declarations.tex}


%--
%	INDEX AND GLOSSARY
%--


\usepackage[style=long,nonumberlist,toc,xindy,acronym,nomain]{glossaries} % nomain, if you define glossaries in a file, and you use \include{INP-00-glossary}
%\loadglsentries[main]{glossary}
% or using \input:
\include{glossary}
\makeglossaries
\usepackage{makeidx}
\makeindex



%==
%	BEGIN DOCUMENT
%==


\begin{document}

% roman numbering 
\setcounter{page}{1}
\pagenumbering{roman}

% print title
\input{./includes/title.tex}

%pagestyle after title
\pagestyle{fancy}

% table of contents,list of figures and list of tables
\setcounter{tocdepth}{1}
% \tableofcontents
% \listoffigures
% \listoftables
{\tableofcontents \let\cleardoublepage\clearpage \listoffigures 
\let\cleardoublepage\clearpage \listoftables \let\cleardoublepage\clearpage}

%\cleardoublepage

% ABSTRACT EXECUTIVE SUMMARY

\begin{abstract}
    This work investigates a novel application of a Vision Transformer to quality 
    analysis of generated images from neural image compression. The Vision Transformer (ViT) 
    is a revolutionary implementation of the Transformer attention mechanism 
    (typically used in language models) to object detection in digital images. 
    Since the ViT architecture is designed to output classification probability distribution, 
    it can be a suitable candidate for quality assessment of generated images based on 
    object-level deviations from the original pre-compression image, rather than measuring 
    per-pixel discrepancies or structural aberrations. Neural image compression and generation is achieved 
    using a Generative Adversarial Network (GAN). This study demonstrates the coveted end-to-end 
    deep learning pipeline for image compression, latent vector representation, regeneration, 
    and quality analysis using state-of-the-art model architectures. Results from this work show 
    that a Vision Transformer is capable of evaluating the quality of a compressed image and compares 
    to established perceived quality metrics such as Structural Similarity (SSIM) and Mean Squared Error (MSE). 
\end{abstract}

\pagestyle{fancy}

\setcounter{page}{1}
\pagenumbering{arabic}


%==
%	CHAPTER 1
%==

\chapter{Introduction to Vision Transformers}

% Chapter overview 
This chapter will present an introduction to Vision Transformers (ViT).

It will cover the motivation as to why ViT, or a future development inspired by it,
will have a profound impact on image compression, analysis, and generation overall.
Evidence will be shown that Vision Transformers, or perhaps an evolved deep learning model with a
similar architecture, i.e. generalizable and highly overparameterized, can be superior in
compressing and evaluating the latent feature space of a digital image compared to known
technologies.

The chapter will then review a brief history or Transformer usage in deep learning.
These generalized architectures are dominating state-of-the-art language models,
as they are extremely efficient in packing information within a one dimensional vector.

This introduction will further describe the principles of operation of a ViT. Then, proceed with
a mathematical formulation. Finally, it will cover currently available implementations
in the form of pre-trained models and conclude with an explanation on the computational and
financial constraints of training such architectures.

\newpage
\input{./includes/chapter1.tex}


%==
%	CHAPTER 2
%==
\cleardoublepage
\chapter{Literature Review: ViT and Neural Image Compression}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\newpage
\input{./includes/chapter2.tex}

%==
%	CHAPTER 3
%==
\cleardoublepage
\chapter{ViT Assessment of Neural Image Compression (Theory and Practice)}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\newpage
\input{./includes/chapter3.tex}

%==
%	CHAPTER 4
%==
\cleardoublepage
\chapter{Discussion}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\newpage
\input{./includes/chapter4.tex}

%==
%	CHAPTER 5
%==
\cleardoublepage
\chapter{Summary}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\newpage
\input{./includes/chapter5.tex}

%==
%	BIBLIOGRAPHY
%==

\cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Bibliography} % add entry to table of contents
\pagestyle{plain}


%{\textbf{\LARGE{Bibliography}}}\\	%headline
%\nocitep{*} % Show all Bib-entries (DEBUG)

\bibliographystyle{abbrvnat}
% \bibliography{bib/algorithm.bib,bib/resDes.bib} % for a better structure you can split your bib items into seperate files
\bibliography{bib/resDes.bib}

\cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Appendix} % add entry to table of contents
\pagestyle{plain}
\input{./includes/appendix.tex}

\end{document}