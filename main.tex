%==
%	DOCUMENT DEFINITION
%==

\documentclass[pdftex,11pt,titlepage,twoside,openright]{report}	


% for suporting multi platform
\usepackage[utf8x]{inputenc} 	

% natbib is what you want for bibliography
\usepackage[square,authoryear]{natbib}


%--
%	DECLARATIONS
%--

\input{./includes/declarations.tex}


%--
%	INDEX AND GLOSSARY
%--


\usepackage[style=long,nonumberlist,toc,xindy,acronym,nomain]{glossaries} % nomain, if you define glossaries in a file, and you use \include{INP-00-glossary}
%\loadglsentries[main]{glossary}
% or using \input:
\include{glossary}
\makeglossaries
\usepackage{makeidx}
\makeindex



%==
%	BEGIN DOCUMENT
%==


\begin{document}

% roman numbering 
\setcounter{page}{1}
\pagenumbering{roman}

% print title
\input{./includes/title.tex}

%pagestyle after title
\pagestyle{fancy}

% table of contents,list of figures and list of tables
\setcounter{tocdepth}{1}
% \tableofcontents
% \listoffigures
% \listoftables
{\tableofcontents \let\cleardoublepage\clearpage \listoffigures 
\let\cleardoublepage\clearpage \listoftables \let\cleardoublepage\clearpage}

%\cleardoublepage

% ABSTRACT EXECUTIVE SUMMARY

\begin{abstract}
    This work investigates a novel application of a Vision Transformer to quality 
    analysis of generated images from neural image compression. The Vision Transformer (ViT) 
    is a revolutionary implementation of the Transformer attention mechanism 
    (typically used in language models) to object detection in digital images. 
    Since the ViT architecture is designed to output classification probability distribution, 
    it can be a suitable candidate for quality assessment of generated images based on 
    object-level deviations from the original pre-compression image, complementing the measurement 
    of per-pixel discrepancies or structural comparisons. Neural image compression and generation is achieved 
    using a Generative Adversarial Network (GAN). This study demonstrates the coveted end-to-end 
    deep learning pipeline for image compression, latent vector representation, regeneration, 
    and quality analysis using state-of-the-art model architectures. Results from this work show 
    that a Vision Transformer is capable of assessing the quality of a compressed image and compares 
    to established perceived quality metrics such as Structural Similarity (SSIM) and Mean Squared Error (MSE). 
    
    \ThinHRule
\end{abstract}

\pagestyle{fancy}

\setcounter{page}{1}
\pagenumbering{arabic}


%==
%	CHAPTER 1
%==

\chapter{Introduction to Vision Transformers}


% Chapter overview 
This chapter will present an introduction to Vision Transformers (ViT).

It will cover the motivation as to why ViT, or a future development inspired by it,
will have a profound impact on image compression, analysis, and generation overall.
Evidence will be shown that Vision Transformers, or perhaps an evolved deep learning model with a
similar architecture, i.e. generalizable and highly overparameterized, can be superior in
compressing and evaluating the latent feature space of a digital image compared to known
technologies.

The chapter will then review a brief history or Transformer usage in deep learning.
These generalized architectures are dominating state-of-the-art language models,
as they are extremely efficient in packing information within a one dimensional vector.

This introduction will then proceed to describe the principles of operation of a ViT. Then, proceed with
a mathematical formulation. Finally, it will cover currently available implementations
in the form of pre-trained models and conclude with an explanation on the computational and
financial constraints of training such demanding architectures.

\ThinHRule

\newpage
\input{./includes/chapter1.tex}


%==
%	CHAPTER 2
%==
\cleardoublepage
\chapter{Literature Review: Transformers and Neural Image Compression}


This chapter serves to present the audience with a literature review of seminal academic
publications relating Transformers to image compression and generation.

Among many, the reader would find interest in the takeaways offered from:

\begin{itemize}
	\item "An Image is Worth 16x16 Words" \citep{dosovitskiy2020vit}
	\item "Towards End-to-End Image Compression and Analysis with Transformers" 
    
    \citep{Bai2022AAAI}
	\item "TransGAN: Two Pure Transformers Can Make One Strong GAN" \citep{jiang2021transgan}
	\item "First Principles of Deep Learning and Compression" \citep{Principles}
	\item Review of commonly used Image Quality Assessment (IQA) metrics \citep{BRISQUE}
    
    \citep{Metrics}
\end{itemize}

\ThinHRule

\newpage
\input{./includes/chapter2.tex}

%==
%	CHAPTER 3
%==
% \cleardoublepage
\chapter{ViT-based Assessment of Neural Image Compression (Theory and Practice)}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\ThinHRule

\newpage
\input{./includes/chapter3.tex}

%==
%	CHAPTER 4
%==
% \cleardoublepage
\chapter{Discussion}

Increase in recent publications (13 in 2022 alone) supports the vision of tying vision transformers to image compression.

\ThinHRule

\newpage
\input{./includes/chapter4.tex}

%==
%	CHAPTER 5
%==
\cleardoublepage
\chapter{Summary}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

\ThinHRule

\newpage
\input{./includes/chapter5.tex}

%==
%	BIBLIOGRAPHY
%==

\cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Bibliography} % add entry to table of contents
\pagestyle{plain}


%{\textbf{\LARGE{Bibliography}}}\\	%headline
%\nocitep{*} % Show all Bib-entries (DEBUG)

\bibliographystyle{abbrvnat}
% \bibliography{bib/algorithm.bib,bib/resDes.bib} % for a better structure you can split your bib items into seperate files
\bibliography{bib/resDes.bib}

\cleardoublepage
\phantomsection %hyperref package support
\addcontentsline{toc}{chapter}{Appendix} % add entry to table of contents
\pagestyle{plain}
\input{./includes/appendix.tex}

\end{document}