\section{Key Contributions}

(experimentation with Vision Transformers, nascent field)
The main merit of this thesis was introducing the ViT-Score.
A Vision Transformer-Assisted metric for evaluating the performance of a neural image compression, the score
correlated with the Generative Adversarial Network verifiably generating a comprehensible image by visual inspection.

The ViT-Score conclusively contributed insights to image quality, relating to human perception of the generated image. 
The metric can also provide additional insights to understanding the latent space (contextual) preservation of an input image.

This work can be viewed as a contribution towards:

\begin{itemize}
    \item an end-to-end Deep Learning-based image compression and reconstruction 
    \item abstracted evaluation of GAN generated images 
	\item promoting generalizable models in Artifical Intelligence (AI) 
	\item expansion of human consciousness through investigation of evolving Deep Learning techniques
\end{itemize} 


\section{Summary}

Throughout this report, the reader was presented with all relevant background knowledge necessary 
to grasp the key contributions listed. 
A Vision Transformer (ViT) was used to evaluate the capacity of 
a GAN to compress and generate an image of choice based on object-level similarities with the original input image.

The new metric, referred to as a ViT-Score, was able to capture and assess the quality of the output images and provide 
valuable insights. The ViT-Score performed well, comparing in capacity to established image quality metrics such as
SSIM, MSE, and PSNR. 


\section{Takeaways}

The future of image compression technology will be based on a Deep Learning methodology.
Due to their generalizability, excellent performance in 1-dimensional data (text),
and proven ability to scale to 2-dimensions, Transformers are an excellent choice of 
architecture to use in image compression and reconstruction.


A Vision Transformer (ViT)-Assisted metric related to image compression can provide 
additional insights to the latent space (contextual) preservation.
Thus, this work can be viewed as a stepping stone towards an end-to-end Transformer-based
image compression and regeneration.


Perhaps, such a metric could be used as a Loss function, embedded within the architecture,
along with being a useful evaluation metric.


It may cost on the order of \$100M and several years, but such a technology will be achieved.


Finally, this image technology can be extended to videos and video compression in further developments.


\section{Acknowledgments}

The author would like to express gratitude towards several individuals and organizations
from The University of Texas at Austin campus.


The major inspiration for this project was gathered from two courses taught by the
reviewers of this thesis.


EE 371Q, Digital Image Processing taught by Professor Dr. Alan C. Bovik was the class
where the author learned about 
Image Compression, Image Quality Assessment, and completed a term
project on Generative Adversarial Networks (GANs).


CSE 382, Foundations of Machine Learning taught by Professor Dr. Rachel A. Ward 
was the class where the author learned key concepts used throughout this thesis
and completed a term project on Vision Transformers (ViT).


Further acknowledgments are made to the Laboratory for Image and Video Engineering (LIVE) at the 
University of Texas at Austin for providing a source for project inspiration and insights.


Finally, The Texas Advanced Computing Center (TACC) provided free access to advanced 
High-Performance Computing (HPC) resources, which were used throughout the experimentation process in this thesis.

\section{Closing Remarks}

This thesis is written as a graduation requirement for the degree of Master of Science in

Computational Science, Engineering, and Mathematics awarded by the Oden Institute at 
The University of Texas at Austin.



\NOTE{All code has been made available as open source to the general public in the form of a GitHub repository.}