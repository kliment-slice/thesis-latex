\section{Key Contributions}

The main merit of this thesis project is introducing the ViT-Score (see Section 3.3).
It is a Vision Transformer-assisted metric for evaluating the performance of neural image compression. 
It computes a score to compare the reconstructed output from a Generative Adversarial Network (GAN) to the original 
input image.


The ViT-Score conclusively contributes to evaluating image quality by quantifying human perception of recognizable objects in the generated image. 
The metric can also provide additional insights to understanding the latent space (contextual) preservation of an input image.
Finally, the ViT-Score can most likely verify that a GAN has generated a comprehensible image.

This work can also be viewed as contributing towards:

\begin{itemize}
    \item an end-to-end Deep Learning-based approach to image compression and reconstruction 
    \item abstracted evaluation of GAN generated images 
	\item promoting generalizable and overparameterized models in Deep Learning
	\item experimentation with Transformers while still a nascent technology
	\item expansion of human consciousness through investigation of evolving techniques in Artifical Intelligence (AI) 
\end{itemize} 


\section{Summary}

Throughout this report, the reader was presented with all relevant background knowledge necessary 
to grasp the key contributions listed. 


A Vision Transformer (ViT) was used to evaluate the capacity of 
a GAN to compress and generate an image of choice based on object-level similarities with the original input image.


The new metric, referred to as a ViT-Score, was able to capture and assess the quality of the output images and provide 
valuable insights. The ViT-Score performed well, comparing in capacity to established image quality metrics such as
SSIM, MSE, and PSNR. 


\section{Takeaways}

The future of image compression technology will be based on a Deep Learning methodology.
Due to their generalizability, excellent performance in 1-dimensional data (text),
and proven ability to scale to 2-dimensions, Transformers are an excellent choice of 
architecture to use in image compression and reconstruction.


A Vision Transformer (ViT)-Assisted metric related to image compression can provide 
additional insights to understanding latent feature space and its preservation through lossy compression.


Such a metric could be used as a Loss function, embedded within the architecture.
It could be useful as a standalone evaluation metric.


It may cost on the order of \$100M and several years to develop, but an end-to-end deep learning-based approach
to image compression will be achieved.


Finally, such image compression technology can be extended to video and video compression in further developments.


\section{Acknowledgments}

The author would like to express gratitude towards several individuals and organizations
from The University of Texas at Austin campus.
The major inspiration for this project was gathered from two courses taught by the
reviewers of this thesis.


EE 371Q, Digital Image Processing taught by Professor Dr. Alan C. Bovik was the class
where the author learned about 
Image Compression, Image Quality Assessment, and completed a term
project on Generative Adversarial Networks (GANs).


CSE 382, Foundations of Machine Learning taught by Professor Dr. Rachel A. Ward 
was the class where the author learned key concepts used throughout this thesis
and completed a term project on Vision Transformers (ViT).


Further acknowledgments are made to the Laboratory for Image and Video Engineering (LIVE) at the 
University of Texas at Austin for providing a source for project inspiration and insights.

Finally, The Texas Advanced Computing Center (TACC) provided free access to advanced 
High-Performance Computing (HPC) resources, which were used throughout the experimentation process in this thesis.

\section{Closing Remarks}

This thesis is written as a graduation requirement for the degree of Master of Science in
Computational Science, Engineering, and Mathematics awarded by the Oden Institute at 
The University of Texas at Austin.

\vspace{5mm}

\NOTE{All code has been made available as open source to the general public in the form of a GitHub repository.}