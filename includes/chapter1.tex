%--
%	CHAPTER 1
%--

\section{Motivation}

Transformers are presently considered to hold a great promise for the future of Deep Learning
as a step towards Artificial General Intelligence.
Due to their architecture, they are more generalizable, less prone to overfitting, and able 
to learn highly complex representations. The Transformer architecture has already been proven 
to make obsolete Recurrent Neural Networks (RNNs) in natural language models and outperform
certain Convolutional Neural Networks (CNNs) in image classification tasks. \citep{dosovitskiy2020vit}


\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{media/papersWcode_tfUsage2.png}
	\end{center}
	\caption[Historical Usage of ViT in Image Tasks]{As of 2022, the usage of a Vision Transformer (ViT) in image 
	tasks matches the usage of ResNets and has outnumbered any other popular CNN architecture.
	\citep{PapersOverTime}}
	\end{figure}

Furthermore, evaluating output quality from Generative Adversarial Networks (GANs) is 
still a developing field using non-Deep Learning-adapted assessment methods. 
For the purpose of this thesis, a GAN was used as a placeholder 
for a future, coveted, and highly desirable Deep Learning-based image compression mechanism. 


This thesis explores a Vision Transformer (ViT)-Assisted metric related to 
image compression, which can provide 
additional insights to GAN output quality and the input latent space (contextual) preservation.


\NOTE{Thus, this work can be viewed as a stepping stone towards an end-to-end Transformer-based
image compression and regeneration.}


\section{Brief History}

\subsection{Attention and Language Models}

"Attention Is All You Need" is a seminal research publication
by a team of Google researchers, which kickstarted the Transformer revolution in Deep Learning in 2017.
It proposes a novel architecture, which models long-range dependencies in
sequential (text) data, by arranging a set of self-attention layers. 

A self-attention layer is what the model uses to focus on different elements of the 
input sequence simultaneously. For example, it can be ysed to compute the 
distance (relationship) between every word in a given sentence. \citep{Attention}


Examples of implementations of text-based Transformers are BERT by Google and GPT-3 by OpenAI.
BERT, among other things, as of 2021 processes and autofills every single English-based 
Google user search query.
GPT-3, on the other hand, revolutionized text generation in 2020, demonstrating the ability 
to generate extremely cohesive textual output.

Most Transformers are used in language modeling and Natural Language Processing (NLP).
Thus, they are often benchmarked against Recurrent Neural Networks (RNNs, and specifically 
Long Short-Term Memory, LSTM architecture). LSTMs rely on hidden states to pass information 
along sequentially during the encoding and decoding process for each word token. 
However, they typically fall short learning long-range dependencies.


\subsection{Attention in Vision Tasks}

The attention mechanism is capable of focusing on objects anywhere on the image 
within a single network layer versus the variable size convolution kernels 
across the different layers.

Tokenization happens at pixel level, 
so each pixel would have to attend to each other pixel in the grid, which becomes 
too heavy to compute, on the order of $(250^2)^2$. To resolve that, the image is 
broken down into blocks of equal size, a 16x16 subset of the image called image patches. 
Then, unroll each image patch into a sequence (256x1), and index it with a positional 
embedding in a table. All of that is then fed into a standard Transformer, 
like from Attention is all you need. Finally, a feed forward classifier (MLP) 
makes the classification prediction, voila image recognition.

Unlike variable size convolution kernels accross 
layers, the block size in the image transformer is able to pay attention within 
a single layer to anywhere on the image. 
CNNs have good inductive priors and can learn any function. 
However, this promotes locality (i.e. nearing pixels are probably most important).
One way to think of a ViT is a generalization of an MLP which itself is a 
generalization of a CNN. The ViT also learns very similarly to a CNN 
(e.g. filters with principal components).

Transformer, in a way, is a generalization of a feed forward network, but 
instead of fixed connections weights in an MLP, each connection weight (i.e. attention) 
is computed on the fly. That makes the Transformer, unlike the MLP, permutation invariant. 
In that it wouldn't know where information is coming from unless there are additional 
learnable sequential positional embeddings, i.e. number down the image patches.


\section{Principles of Operation}

This section will elaborate the key principles of operation of a ViT.

Close attention is paid to certain steps of 
the encoding process (like certain key words in sentence or objects in image blocks). 
In effect, the Decoder outputs Keys at each step which index the hidden states 
(and will explain more intuition about the Keys when upon formulating attention). 
It uses a Softmax architecture to normalize and map the potential output classes 
to a probability distribution. Positional encoding is achieved with trig functions, 
e.g. a full sentence or perhaps an image dimension could be represented by the various
periods of a sin wave, so the exact location of each token would be unique.

Sets of parallel attention layers at each token are called multi-head attention 
(to vary what to pay attention to: e.g. at verbs in the sentence or different objects 
in an image). The multi-head attention is composed of Key-Value pairs coming from 
the encoding part of the source sentence or image (i.e. the input embedding) and 
Queries from the output embedding (i.e. encoding part of target sentence or image).
Now that we explained relevant transformer components, we can see how it applies to 
2D signals, i.e. image matrices for classification purposes in image recognition.


\section{Mathematical Formulation}

So in its full formulation, Attention is a function of queries, keys and values 
vectors labeled capital $(Q,K,V)$. 
It equals the dot product $(QK^T)$ of keys and queries respectively, softmaxed over 
the square root of dimensions and multiplied by Values.
So:
Values - are what is most interesting in the source (sentence or image), 
e.g. attributes or features (like keyword adjectives or perhaps structural 
features in an image).
Keys - index (or address) those values (name, type, weight). Each key has an 
associated value. Queries - are built by the encoder of the target sentece or 
image and prompt the network to find information. 

\begin{center}
	$ Attention(Q, K, V) = softmax(\dfrac{QK^T}{\sqrt{d_k}} V ) $
	\end{center}

The overall dynamic: a Query is pegged against a Key to locate a certain Value.	

The softmax is basically a normalized exponential function: sequence of Variables is 
mapped into exponentials and divided by the sum of all the exponentials. Thus, the 
large numbers become almost ones and small numbers near zeros, like the maximum 
function, but this one is differentiable. 

\begin{center}
	$ \displaystyle\sigma(Z_i) = \dfrac{e^{z_j}}{\sum_{j=1}^K e^{z_j}}$ for $i=1,...,K$ $and z=(z_1,...,z_K) \in \mathbb{R}^K $
	\end{center}

So a softmax of an inner product of each key with query vector normalizes to a 
probability distribution over all Values (very similar to using a softmax in the last 
layer of a NN over all the labels to yield the top classification pick). 


To understand vector proximity between embeddings, e.g. similarity in objects for images, or words in sentences.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{media/dot_real.png}
	\end{center}
	\caption[Vector Representation]{Vector representation on a unit circle.}
	\end{figure}

The dot product of Keys and Queries yields an angle between both vectors (e.g. $u_i$ and $u_j$ in Figure 1.2 above) to measure 
how similarly aligned they are. In high dimension, most vectors would be orthonormal 
and $cos(90)=0$. But if Key and Query align, they'd have a large dot product. 
Each key in space has an associated value (the pair). The Query vector is computed with 
each key and softmaxed to select one Key with the highest dot product. With softmax, 
a certain Key will stand out (in magnitude) vs the rest.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{media/dot2_real.png}
	\end{center}
	\caption[Key/Query Vector Proximity]{Vector proximity shows the closest Key vector to a given Query vector.}
	\end{figure}

So the formulation of each proximity is the dot product of vectors K with Q:
\begin{center}
	$ \displaystyle sum(\langle K_z \vert Q \rangle) $
	\end{center}




\newpage

\section{Implementations}
\subsection{Open Source Pre-trained Models}

Open Source
On GitHub.
Original one by Google in tensorflow.
Hugging Face in PyTorch.
For the purpose of this thesis, we will use a PyTorch implementation trained on ImageNet-21k
and fine tuned on ImageNet-1k.

\subsection{Closed Source}

OpenAI (DALL.E 2) released in April 2022, trained on 250M image-text pairs 
to be able to generate images from textual description. \citep{Dalle2}
Not much information, not open source like google's ViT or BERT.

\section{Computational Constraints}

Time, memory, cost

Text transformers perform extremely well on orders of magnitude smaller size training examples.
GPT-3 trained on 45TB of text data (Wikipedia included), has 175B parameters and 96 attention layers. \citep{GPT3}
\$4.6M using a Nvidia advanced datacenter GPU grade cloud cluster.

Several orders of magnitude more for image data.

Need a ViT on all internet, to cost \$100M in training alone. 
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops








