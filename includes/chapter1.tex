%--
%	CHAPTER 1
%--

\section{Motivation}

Transformers are presently considered to hold a great promise for the future of Deep Learning
as a step towards Artificial General Intelligence.
Due to their architecture, they are more generalizable, less prone to overfitting, and able 
to learn highly complex representations. The Transformer architecture has already been proven 
to make obsolete Recurrent Neural Networks (RNNs) in natural language models. Furthermore, the 
Vision Transformer (ViT) has outperformed certain Convolutional Neural Networks (CNNs) in image classification tasks. \citep{dosovitskiy2020vit}

Figure 1.1 below shows an increase in the popularity of research related to Vision Transformers.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{media/papersWcode_tfUsage2.png}
	\end{center}
	\caption[Historical Usage of ViT in Image Tasks]{As of 2022, the usage of a Vision Transformer (ViT) in image 
	tasks matches the usage of ResNets and has outnumbered any other popular CNN architecture.
	\citep{PapersOverTime}}
	\end{figure}

Figure 1.1 was produced by PapersWithCode, a popular academic research aggregator. For the past three years, 
ResNets, the most popular architecture in image processing and computer vision, has dominated the proportion
of academic research in object detection. In 2022, Vision Transformer research popularity has reached
that of ResNets and exceeded any other major category.


In the zeitgeist of Vision Transformer research, this thesis explores a ViT-assisted metric related to 
image compression. This metric can provide additional insights to GAN output quality 
and the latent space (contextual) preservation of a variety of input images.


\NOTE{Thus, contributions from this thesis can be viewed as providing a stepping stone 
towards an end-to-end Transformer-based image compression and reconstruction framework.}


\section{Brief History}

\subsection{Attention and Language Models}

"Attention Is All You Need" is a seminal research publication
by a team of Google researchers, which kickstarted the Transformer revolution in Deep Learning in 2017.
It proposes a novel architecture, which models long-range dependencies in
sequential (text) data, by arranging a set of self-attention layers. 

A self-attention layer is what the model uses to focus on different elements of the 
input sequence simultaneously. For example, it can be used to compute the 
distance (relationship) between every word in a given sentence. \citep{Attention}


Examples of implementations of text-based Transformers are BERT by Google and GPT-3 

by OpenAI.
BERT, among many other applications, processes and autofills every single English-based 
Google user search query as of 2021 . \citep{bert}
GPT-3, on the other hand, revolutionized text generation in 2020, demonstrating the ability 
to generate extremely cohesive textual output.

Most Transformers are used for applications in language modeling and Natural Language 

Processing (NLP).
Thus, they are often benchmarked against Recurrent Neural Networks (RNNs, and specifically 
Long Short-Term Memory, LSTM architecture). LSTMs rely on hidden states to pass information 
along sequentially during the encoding and decoding process for each word token. 
However, they typically fall short learning long-range dependencies.


\subsection{Attention in Vision Tasks}

The attention mechanism is capable of focusing on objects found anywhere on an input image.
It operates within a single network layer compared to Convolutional Neural Networks (CNNs),
where the variable size convolution kernels scan across the different layers of the architecture.
\citep{dosovitskiy2020vit}

As shown in Figure 1.2 below, tokenization happens at the pixel level, i.e. each pixel attends to each other pixel in the grid. 
This becomes computationally intensive, on the order of $(n^2)^2$, where $n$ denotes width of a square image. 
To resolve this, the input image is broken down into square blocks of equal size, referred to as image patches.
Then, each image patch is unrolled into a one-dimensional sequence $(n x 1)$ and indexed with a positional 
embedding in a table for future reference and retrieval purposes. The embeddings enter the Transformer and 
finally, a feed forward classifier, in the form of a Multilayer Perceptron (MLP) 
makes the classification prediction, yielding a probability distribution. \citep{dosovitskiy2020vit}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{media/vit_arch.png}
	\end{center}
	\caption[ViT Architecture]{The Vision Transformer (ViT) architecture. \citep{GoogleViT}}
	\end{figure}

A Transformer, in a way, is a generalization of a feed forward network, but 
instead of fixed connections weights in an MLP, each connection weight (i.e. attention) 
is computed ad hoc. This makes the Transformer, unlike the MLP, permutation invariant. 
That is, it would not know where certain information is coming from, unless there are additional 
learnable sequential positional embeddings, i.e. index the image patches.

\section{Principles of Operation}

Continuing from the previous section, a ViT can be thought of as a 
generalization of an MLP, which itself is a generalization of a CNN. 
The ViT happens to learn very similarly to a CNN, which represents the latent space
as filters carrying principal components.

In principle, CNNs have good inductive priors and can learn any function. 
However, they promote locality, i.e. nearing pixels are probability-wise considered most important.
This may easily not be desired, especially in the key applications of object detection and, in the 
future, image compression.


The encoding process indexes embeddings. For instance, certain key words in a sentence or 
objects in image blocks are mapped in a reference lookup table. 
The Decoder outputs Keys at each step. These vectors represent hidden states,
which are being passed on into each next iteration of the Transformer. 
The last layer, expectedly, uses a Softmax architecture to normalize and map the potential output classes 
to a probability distribution. 

%\vspace{10mm}
\newpage

\textbf{Multi-Head Attention}


As shown in Figure 1.3 below, sets of parallel attention layers at each token are called multi-head attention.
This approach varies what to pay attention to, for example,at the different objects 
in an image (or in natural langue, different verbs in a sentence). 
The multi-head attention is composed of Key-Value pairs coming from 
the encoding part of the source image (i.e. the input embedding) and 
Queries from the output embedding (i.e. encoding part of target image).

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{media/attn_diag.png}
	\end{center}
	\caption[Attention Mechanism]{The Attention mechanism architecture. \citep{GoogleViT}}
	\end{figure}


\section{Mathematical Formulation}

\textbf{Attention}


In its full formulation, Attention is a function of vectors representing Queries, Keys, and Values, 
labeled as $(Q,K,V)$. 
Attention equals the dot product $(QK^T)$ of Keys and Queries respectively, softmaxed over 
the square root of dimensions and multiplied by the Values vector.

To provide intuition:

Values are what is most interesting in the source image, i.e. 
attributes or structural features. In text, a Value could be important adjectives before each keyword, 
which provide emphasis in a given input sentence.
Keys, on the other hand, index (or address) those Values (e.g. name, type, weight). 
Each Key has an associated Value. 
Queries are built by the encoder of the target image and prompt the network to find closest 
available information (Key and its corresponding Value). 

\begin{center}
	$ Attention(Q, K, V) = softmax(\dfrac{QK^T}{\sqrt{d_k}} V ) $
	\end{center}


\NOTE{Thus, the overall dynamic is that a Query is pegged against a Key to locate a certain Value.}

\vspace{10mm}

\textbf{Softmax}


The Softmax function is defined as a normalized exponential function. 
A sequence of variables is mapped into exponentials and divided by the sum of all exponentials. 
Thus, the large numbers become almost ones and small numbers near zeros. Softmax is similar to the maximum 
function, with a key difference that Softmax is differentiable. 

\begin{center}
	$ \displaystyle\sigma(Z_i) = \dfrac{e^{z_j}}{\sum_{j=1}^K e^{z_j}}$ for $i=1,...,K$ $and z=(z_1,...,z_K) \in \mathbb{R}^K $
	\end{center}

Thus, a Softmax of an inner product of each Key with Query vector normalizes to a 
probability distribution over all Values. Neural networks typically utilize a softmax in their last 
layer over all the classification labels. This yields the top classification by probability. 
Using the softmax function, a certain Key will stand out (in magnitude) vs the rest.

\vspace{10mm}

\textbf{Vector Similarity}


Vector proximity between embeddings represents the similarity between objects in images, or 
word connotations in sentences.


\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{media/dot_real.png}
	\end{center}
	\caption[Vector Representation]{Vector representation on a unit circle.}
	\end{figure}


The dot product of Keys and Queries yields an angle between both vectors (e.g. $u_i$ and $u_j$ in Figure 1.2 above) to measure 
how similarly aligned they are. In high dimensional spaces, most vectors would be orthonormal to each other
and $cos(90)=0$. But if Key and Query vectors are similar or align, they'd have a large dot product. 
The larger the similarity, the larger the dot product. 
The Query vector is computed with each Key in the surrounding vector space and softmaxed to 
select the one Key with the highest dot product.
The selected Key in space has an associated Value. In applications, that Value would correspond to 
a match with a labeled object in an image, or perhaps the next word in a generated sentence. 


A depiction of the vector space is shown in Figure 1.3 below.


\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{media/dot2_real.png}
	\end{center}
	\caption[Key/Query Vector Proximity]{Vector proximity shows the closest Key vector to a given Query vector in space.}
	\end{figure}


\section{Implementations}

This section reviews several notable ViT implementations of interest.
Most developments have been made open source. Howerver, some of the 
highest quality implementations are still closed source and operated 
under a license or payment wall.

\subsection{Open Source}

The academic research aggregator PapersWithCode lists 96 open source implementations of 
the original ViT from "An Image is Worth 16x16 Words" discussed in Chapter 2. 
The publication was made for ICLR 2021. \citep{Papers16}

The original model from the team of Google researchers, written using TensorFlow, has 72,567 stars in its GitHub repository.
The second highest implementation by Hugging Face, written using PyTorch, has 61,820 stars. 


\vspace{5mm}

\textbf{Pre-trained Model Used}

For the purpose of this thesis, a PyTorch implementation was used trained on ImageNet-21k
and fine tuned on ImageNet-1k. The ViT has 9,696 stars on it's GitHub repository,
ranking as the 5th highest rated implementation. It was chosen due to its reliability,
project maturity, and interfaceability. \citep{Papers16}

\vspace{5mm}

A python pip package is also available on PyPI from the same "lucidrains" implementation,
which can be installed via: 
\begin{verbatim} "pip install pytorch_pretrained_vit" \end{verbatim}


Other pre-trained models are also availabe on GitHub and Google Colab. 
Some suffer from a lower GitHub star rating, are not well packaged (not object-oriented), or perhaps deviate too far
from the original implementation of "An Image is Worth 16x16 Words". 


\subsection{Closed Source}


Several modified implementations targeting specific generative applications have been made by Hugging Face and OpenAI.
OpenAI has expanded from its success from GPT-3, a text generation Transformer, to visual applications.

Transformers and Vision Transformers can be used to find context in image objects from the latent space of an input image.

CLIP, Image GPT, DALL.E, and DALL.E2 by OpenAI bridge the gap between textual information and images.
DALL.E 2, the highest quality image Transformer to date, was released in April 2022.
It is trained on 250M image-text pairs to be able to generate coherent images from textual description. \citep{Dalle2}

\vspace{1mm}

However, most of OpenAI's Transformers (e.g. GPT-3, DALL.E 2) are not open source like Google's ViT or BERT.


\section{Computational Constraints for Training}


The main reason for using a pre-trained ViT is that the Transformer architecture is so generalizable and 
overparameterized, that the computational requirements for Time, Memory, and Cost are beyond the scope of this thesis.


For example, GPT-3 was trained on 45TB of text data (all of Wikipedia included). It has 175B parameters and 96 attention layers. \citep{GPT3}
One flavor of GPT-3 (of many) alone cost \$4.6M to train using the most advanced Nvidia datacenter GPUs in a cluster.

\vspace{10mm}

\textbf{Training the Next Generation of Image GPT}

It would require several orders of magnitude more resources to train a Vision Transformer that matches the quality of GPT-3.

The training data would need to be similar to all Google images on the internet (or at least a respectable representative subset).
It would probably cost on the order of \$50M in training alone, since image matrices are two-dimensional sequences of data.
Contextual complexity also rises exponentially for images compared to text.










