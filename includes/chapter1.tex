%--
%	CHAPTER 1
%--

\section{Motivation}

Transformers are presently considered to hold a great promise for the future of deep learning
as a step towards Artificial General Intelligence.
Due to their architecture, they are more generalizable, less prone to overfitting, and able 
to learn highly complex representations. The Transformer architecture has already been proven 
to make obsolete Recurrent Neural Networks (RNNs) in natural language models and outperform
Convolutional Neural Networks (CNNs) in image classification tasks.

A Vision Transformer (ViT)-Assisted metric related to image compression can provide 
additional insights to the latent space (contextual) preservation.
Thus, this work can be viewed as a stepping stone towards an end-to-end Transformer-based
image compression and regeneration.


\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{media/papersWcode_tfUsage2.png}
	\end{center}
	\caption[Historical Usage of ViT in Image Tasks]{As of 2022, the usage of a Vision Transformer (ViT) in image 
	tasks matches the usage of ResNets and has outnumbered any other popular CNN architecture.
	\citep{PapersOverTime}}
	\end{figure}


\section{Brief History}

\subsection{Language Models: BERT, GPT-3}

Introduced in 2017 by google. Mostly used for NLP (e.g. openAI GPTs, 
which all heard of or google BERT, which as of recently processes 
and autofills every single English-based google search query). 
They are often benchmarked against RNNs (specifically LSTMs), when used in 
language models. LSTMs traditionally rely on hidden states to pass information 
along sequentially during encoding and then decoding each word token. BUT RNNs 
typically fall short learning long range dependencies.
Transformers however use the attention mechanism to weigh the influence of 
different parts of the input.

\subsection{Vision Tasks}
In the seminal ICLR 2021 publication "An Image is Worth 16x16 Words" by a Google team,
a transformer architecture trained on ImageNet outperformed or compared to state-of-the art CNNs
with greater efficiency on image classification tasks.
Tokenization happens at pixel level, 
so each pixel would have to attend to  each other pixel in the grid, which becomes 
too heavy to compute, on the order of $(250^2)^2$. To resolve that, the image is 
broken down into blocks of equal size, a 16x16 subset of the image called image patches. 
Then, unroll each image patch into a sequence (256x1), and index it with a positional 
embedding in a table. All of that is then fed into a standard Transformer, 
like from Attention is all you need. Finally, a feed forward classifier (MLP) 
makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.


\section{Principles of Operation}

Attention mechanism \citep{Attention}
Attention is a mechanism where the decoder can go back and look at particular 
parts of input (unlike RNNs). So, close attention is paid to certain steps of 
the encoding process (like certain key words in sentence or objects in image blocks). 
In effect, the Decoder outputs Keys at each step which index the hidden states 
(and will explain more intuition about the Keys when upon formulating attention). 
It uses a Softmax architecture to normalize and map the potential output classes 
to a probability distribution. Positional encoding is achieved with trig functions, 
e.g. a full sentence or perhaps an image dimension could be represented by the various
periods of a sin wave, so the exact location of each token would be unique.

Sets of parallel attention layers at each token are called multi-head attention 
(to vary what to pay attention to: e.g. at verbs in the sentence or different objects 
in an image). The multi-head attention is composed of Key-Value pairs coming from 
the encoding part of the source sentence or image (i.e. the input embedding) and 
Queries from the output embedding (i.e. encoding part of target sentence or image).
Now that we explained relevant transformer components, we can see how it applies to 
2D signals, i.e. image matrices for classification purposes in image recognition.


\section{Mathematical Formulation}

So in its full formulation, Attention is a function of queries, keys and values 
vectors labeled capital $(Q,K,V)$. 
It equals the dot product $(QK^T)$ of keys and queries respectively, softmaxed over 
the square root of dimensions and multiplied by Values.
So:
Values - are what is most interesting in the source (sentence or image), 
e.g. attributes or features (like keyword adjectives or perhaps structural 
features in an image).
Keys - index (or address) those values (name, type, weight). Each key has an 
associated value. Queries - are built by the encoder of the target sentece or 
image and prompt the network to find information. 

\begin{center}
	$ Attention(Q, K, V) = softmax(\dfrac{QK^T}{\sqrt{d_k}} V ) $
	\end{center}

The overall dynamic: a Query is pegged against a Key to locate a certain Value.	

The softmax is basically a normalized exponential function: sequence of Variables is 
mapped into exponentials and divided by the sum of all the exponentials. Thus, the 
large numbers become almost ones and small numbers near zeros, like the maximum 
function, but this one is differentiable. 

\begin{center}
	$ \displaystyle\sigma(Z_i) = \dfrac{e^{z_j}}{\sum_{j=1}^K e^{z_j}}$ for $i=1,...,K$ $and z=(z_1,...,z_K) \in \mathbb{R}^K $
	\end{center}

So a softmax of an inner product of each key with query vector normalizes to a 
probability distribution over all Values (very similar to using a softmax in the last 
layer of a NN over all the labels to yield the top classification pick). 


To understand vector proximity between embeddings, e.g. similarity in objects for images, or words in sentences.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{media/dot_real.png}
	\end{center}
	\caption[Vector Representation]{Vector representation on a unit circle.}
	\end{figure}

The dot product of Keys and Queries yields an angle between both vectors (e.g. $u_i$ and $u_j$ in Figure 1.2 above) to measure 
how similarly aligned they are. In high dimension, most vectors would be orthonormal 
and $cos(90)=0$. But if Key and Query align, they'd have a large dot product. 
Each key in space has an associated value (the pair). The Query vector is computed with 
each key and softmaxed to select one Key with the highest dot product. With softmax, 
a certain Key will stand out (in magnitude) vs the rest.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{media/dot2_real.png}
	\end{center}
	\caption[Key/Query Vector Proximity]{Vector proximity shows the closest Key vector to a given Query vector.}
	\end{figure}

So the formulation of each proximity is the dot product of vectors K with Q:
\begin{center}
	$ \displaystyle sum(\langle K_z \vert Q \rangle) $
	\end{center}




\newpage

\section{Implementations}
\subsection{Open Source Pre-trained Models}

Open Source
On GitHub.
Original one by Google in tensorflow.
Hugging Face in PyTorch.
For the purpose of this thesis, we will use a PyTorch implementation trained on ImageNet-21k
and fine tuned on ImageNet-1k.

\subsection{Closed Source}

OpenAI (DALL.E 2) released in April 2022, trained on 250M image-text pairs 
to be able to generate images from textual description. \citep{Dalle2}
Not much information, not open source like google's ViT or BERT.

\section{Computational Constraints}

Time, memory, cost

Text transformers perform extremely well on orders of magnitude smaller size training examples.
GPT-3 trained on 45TB of text data (Wikipedia included), has 175B parameters and 96 attention layers. \citep{GPT3}
\$4.6M using a Nvidia advanced datacenter GPU grade cloud cluster.

Several orders of magnitude more for image data.

Need a ViT on all internet, to cost \$100M in training alone. 
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops








