%--
%	CHAPTER 1
%--

\section{Motivation}

Transformers are presently considered to hold a great promise for the future of Deep Learning
as a step towards Artificial General Intelligence.
Due to their architecture, they are more generalizable, less prone to overfitting, and able 
to learn highly complex representations. The Transformer architecture has already been proven 
to make obsolete Recurrent Neural Networks (RNNs) in natural language models. Furthermore, the 
Vision Transformer (ViT) has outperformed certain Convolutional Neural Networks (CNNs) in image classification tasks. \citep{dosovitskiy2020vit}

Figure 1.1 below shows an increase in the popularity of research related to Vision Transformers.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{media/papersWcode_tfUsage2.png}
	\end{center}
	\caption[Historical Usage of ViT in Image Tasks]{As of 2022, the usage of a Vision Transformer (ViT) in image 
	tasks matches the usage of ResNets and has outnumbered any other popular CNN architecture.
	\citep{PapersOverTime}}
	\end{figure}

Figure 1.1 was produced by PapersWithCode, a popular academic research aggregator. For the past three years, 
ResNets, the most popular architecture in image processing and computer vision, has dominated the proportion
of academic research in object detection. In 2022, Vision Transformer research popularity has reached
that of ResNets and exceeded any other major category.


In the zeitgeist of Vision Transformer research, this thesis will explore a ViT-assisted metric related to 
image compression. This metric can provide additional insights to GAN output quality 
and the latent space (contextual) preservation of a variety of input images.


\NOTE{Thus, this work can be viewed as a stepping stone towards an end-to-end Transformer-based
image compression and regeneration framework.}


\section{Brief History}

\subsection{Attention and Language Models}

"Attention Is All You Need" is a seminal research publication
by a team of Google researchers, which kickstarted the Transformer revolution in Deep Learning in 2017.
It proposes a novel architecture, which models long-range dependencies in
sequential (text) data, by arranging a set of self-attention layers. 

A self-attention layer is what the model uses to focus on different elements of the 
input sequence simultaneously. For example, it can be ysed to compute the 
distance (relationship) between every word in a given sentence. \citep{Attention}


Examples of implementations of text-based Transformers are BERT by Google and GPT-3 by OpenAI.
BERT, among other things, as of 2021 processes and autofills every single English-based 
Google user search query. \citep{bert}
GPT-3, on the other hand, revolutionized text generation in 2020, demonstrating the ability 
to generate extremely cohesive textual output.

Most Transformers are used in language modeling and Natural Language Processing (NLP).
Thus, they are often benchmarked against Recurrent Neural Networks (RNNs, and specifically 
Long Short-Term Memory, LSTM architecture). LSTMs rely on hidden states to pass information 
along sequentially during the encoding and decoding process for each word token. 
However, they typically fall short learning long-range dependencies.


\subsection{Attention in Vision Tasks}

The attention mechanism is capable of focusing on objects found anywhere on an input image.
It operates within a single network layer compared to Convolutional Neural Networks (CNNs),
where the variable size convolution kernels scan across the different layers of the architecture.
\citep{dosovitskiy2020vit}

Tokenization happens at the pixel level, i.e. each pixel attends to each other pixel in the grid. 
This becomes computationally intensive, on the order of $(n^2)^2$, where $n$ denotes width of a square image. 
To resolve this, the input image is broken down into square blocks of equal size, referred to as image patches.
Then, each image patch is unrolled into a one-dimensional sequence $(n x 1)$ and indexed with a positional 
embedding in a table for future reference and retrieval purposes. The embeddings enter the Transformer and 
finally, a feed forward classifier, in the form of a Multilayer Perceptron (MLP) 
makes the classification prediction, yielding a probability distribution.\citep{dosovitskiy2020vit}


Transformer, in a way, is a generalization of a feed forward network, but 
instead of fixed connections weights in an MLP, each connection weight (i.e. attention) 
is computed ad hoc. That makes the Transformer, unlike the MLP, permutation invariant. 
That is, it would not know where certain information is coming from, unless there are additional 
learnable sequential positional embeddings, i.e. index the image patches.


\section{Principles of Operation}

Continuing from the previous section, a good way to think of a ViT is as a 
generalization of an MLP, which itself is a generalization of a CNN. 
The ViT happens to learn very similarly to a CNN, which represents the latent space
as filters carrying principal components.

In principle, CNNs have good inductive priors and can learn any function. 
However, they promote locality, i.e. nearing pixels are probability-wise considered most important.
This may easily not be desired, especially in the key applications of object detection and, in the 
future, image compression.


The encoding process indexes embeddings. For instance, certain key words in a sentence or 
objects in image blocks are mapped in a reference lookup table. 
The Decoder outputs Keys at each step. These vectors represent hidden states,
which are being passed on into each next iteration of the Transformer. 
The last layer, expectedly, uses a Softmax architecture to normalize and map the potential output classes 
to a probability distribution. 


\textbf{Multi-Head Attention}


Sets of parallel attention layers at each token are called multi-head attention 
(to vary what to pay attention to: e.g. at verbs in the sentence or different objects 
in an image). The multi-head attention is composed of Key-Value pairs coming from 
the encoding part of the source sentence or image (i.e. the input embedding) and 
Queries from the output embedding (i.e. encoding part of target sentence or image).
Now that we explained relevant transformer components, we can see how it applies to 
2D signals, i.e. image matrices for classification purposes in image recognition.


\section{Mathematical Formulation}

So in its full formulation, Attention is a function of queries, keys and values 
vectors labeled capital $(Q,K,V)$. 
It equals the dot product $(QK^T)$ of keys and queries respectively, softmaxed over 
the square root of dimensions and multiplied by Values.
So:
Values - are what is most interesting in the source (sentence or image), 
e.g. attributes or features (like keyword adjectives or perhaps structural 
features in an image).
Keys - index (or address) those values (name, type, weight). Each key has an 
associated value. Queries - are built by the encoder of the target sentece or 
image and prompt the network to find information. 

\begin{center}
	$ Attention(Q, K, V) = softmax(\dfrac{QK^T}{\sqrt{d_k}} V ) $
	\end{center}

The overall dynamic: a Query is pegged against a Key to locate a certain Value.	

The softmax is basically a normalized exponential function: sequence of Variables is 
mapped into exponentials and divided by the sum of all the exponentials. Thus, the 
large numbers become almost ones and small numbers near zeros, like the maximum 
function, but this one is differentiable. 

\begin{center}
	$ \displaystyle\sigma(Z_i) = \dfrac{e^{z_j}}{\sum_{j=1}^K e^{z_j}}$ for $i=1,...,K$ $and z=(z_1,...,z_K) \in \mathbb{R}^K $
	\end{center}

So a softmax of an inner product of each key with query vector normalizes to a 
probability distribution over all Values (very similar to using a softmax in the last 
layer of a NN over all the labels to yield the top classification pick). 


To understand vector proximity between embeddings, e.g. similarity in objects for images, or words in sentences.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.5\textwidth]{media/dot_real.png}
	\end{center}
	\caption[Vector Representation]{Vector representation on a unit circle.}
	\end{figure}

The dot product of Keys and Queries yields an angle between both vectors (e.g. $u_i$ and $u_j$ in Figure 1.2 above) to measure 
how similarly aligned they are. In high dimension, most vectors would be orthonormal 
and $cos(90)=0$. But if Key and Query align, they'd have a large dot product. 
Each key in space has an associated value (the pair). The Query vector is computed with 
each key and softmaxed to select one Key with the highest dot product. With softmax, 
a certain Key will stand out (in magnitude) vs the rest.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{media/dot2_real.png}
	\end{center}
	\caption[Key/Query Vector Proximity]{Vector proximity shows the closest Key vector to a given Query vector.}
	\end{figure}

So the formulation of each proximity is the dot product of vectors K with Q: $ \displaystyle sum(\langle K_z \vert Q \rangle) $

%\newpage

\section{Implementations}
\subsection{Open Source Pre-trained Models}

Open Source
On GitHub.
Original one by Google in tensorflow.
Hugging Face in PyTorch.
For the purpose of this thesis, we will use a PyTorch implementation trained on ImageNet-21k
and fine tuned on ImageNet-1k.

\subsection{Closed Source}

OpenAI (DALL.E 2) released in April 2022, trained on 250M image-text pairs 
to be able to generate images from textual description. \citep{Dalle2}
Not much information, not open source like google's ViT or BERT.

\section{Computational Constraints}

Time, memory, cost

Text transformers perform extremely well on orders of magnitude smaller size training examples.
GPT-3 trained on 45TB of text data (Wikipedia included), has 175B parameters and 96 attention layers. \citep{GPT3}
\$4.6M using a Nvidia advanced datacenter GPU grade cloud cluster.

Several orders of magnitude more for image data.

Need a ViT on all internet, to cost \$100M in training alone. 
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops








