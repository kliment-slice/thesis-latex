\section{"An Image is Worth 16x16 Words"}

\textbf{Transformers for Image Recognition at Scale}


This seminal academic work was published for the International Conference on 
Learning Representations (ICLR) 2020 by a team from Google.
It presents the first Vision Transformer (ViT) for object detection
trained on the ImageNet dataset, a rather large natural images dataset common to research on 
image classification.


Since Chapter 1 already explained and formulated the ViT, this section will focus
on outlining the outcomes from this publication.

"An Image is Worth 16x16 Words" completely discards the notion of convolutions. 
The team used an image patch-based approach as embeddings to a Transformer to classify ImageNet images, 
as opposed to using Convolutional Neural Networks (CNNs), which dominate this field of research.
The ViT model achieves a top-1 accuracy of 77.3\% on ImageNet, which 
compares to the accuracy of state-of-the-art CNNs.

Compared to its convolutional counterpart (the ResNet), the ViT cost 75\% 
less resources to train and outperformed on ImageNet accuracy by 1\%. 
ViT uses approximately 2-4x less compute to attain the same performance 
(averaged over 5 test datasets). 

The total number of parameters is on the order of 100M.

% \begin{figure}[H]
% \begin{algorithm}[H]
% \KwData{A as Array to sort,}
% \KwResult{A as sorted Array} 

% int n $\leftarrow$ A.size \tcp*[l]{cache the initial size of A}


% \Repeat{n>1}{
% 	int newn $\leftarrow$ 1
% 	\For{int i=0; i<n-1; i++}
% 	{
% 		\If{A[i] > A[i+1]}
% 		{
% 			A.swap(i, +1);
% 		}
% 	}
% 	n  $\leftarrow$ newn
% }

% return A
% \caption{bubbleSort(Array A)}
% \end{algorithm}
%\caption[TBC]{Figure caption.v.}
% \end{figure}


\newpage
\section{"End-to-End Image Compression with Transformers"}

Unique positional encoding can also be achieved using trigonometric representation.
For example, a full sentence from text or perhaps a row of pixels from an image could be represented by the various
periods of a sinusoid. Thus, the exact location of each token would be unique.


\section{Image Generation with GANs}

Generative models.
GANs used to be the coolest thing until Transformers came along, but nonetheless 
still great. Typically, GANs pit 2 CNNs against each other, one called Generator to 
generate an image and one called Discriminator to determine how fake the image is 
compared to the input, relaying feedback to improve the Generator after each epoch. 
A GAN pits two Neural Networks (e.g. Convolutional) one against the other to generate 
new content (e.g. an image). The structure of a GAN is foundational to its design. 
As Fig.1 shows, the Generator CNN is shown on the left of the generated output. 
The Generator iteratively learns to create new synthetic data resembling real source data 
(e.g. regular or 360Â° images).  On the right of the generated output is the Discriminator CNN.
The Discriminator is trained to differentiate between real and synthetic data. 
In essence, the Discriminator gives the Generator feedback on its performance while 
simultaneously achieving incremental improvement in discernibility. 
Thus, each epoch the Generator produces more and more realistic data and the Discriminator iteratively improves its own ability to differentiate between real and synthetic data.


\section{First Principles of Neural Image Compression}

Deterministic, Probabilistic
GANs
JPEG/MPEG

\section{Commonly Used Metrics for Image Quality}
Metrics for the generated output image are: a BRISQUE score (reference-less) [5], 
Mean Squared Error (MSE) and Structural Similarity Index (SSIM). PSNR.
GANs use FID and Inception score. 
BRISQUE \citep{BRISQUE}
Divisive normalization, Gaussian properties from natural scene statistics
Process results in Gaussian unless it's distorted
BRISQUE too simple for user-generated content (UGC)