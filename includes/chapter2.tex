\section{"An Image is Worth 16x16 Words"}

\large{Transformers for Image Recognition at Scale}

Now that we explained relevant transformer components, we can see how it applies 
to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a 
google team for a ImageNet trained transformer.

The attention mechanism is capable of focusing on objects anywhere on the image 
within a single network layer versus the variable size convolution kernels 
across the different layers.

"image is worth 16x16 words" completely discards the notion of convolutions. 
Compared to a convolutional counterpart (the ResNet), the ViT cost 75\% 
less to train and beats accuracy on ImageNet by 1\%. 
ViT uses approximately 2-4x less compute to attain the same performance 
(averaged over 5 datasets). Unlike variable size convolution kernels accross 
layers, the block size in the image transformer is able to pay attention within 
a single layer to anywhere on the image. 
CNNs have good inductive priors and can learn any function. 
However, this promotes locality (i.e. nearing pixels are probably most important).
One way to think of a ViT is a generalization of an MLP which itself is a 
generalization of a CNN. The ViT also learns very similarly to a CNN 
(e.g. filters with principal components).
Transformer, in a way, is a generalization of a feed forward network, but 
instead of fixed connections weights in an MLP, each connection weight (i.e. attention) 
is computed on the fly. That makes the Transformer, unlike the MLP, permutation invariant. 
In that it wouldn't know where information is coming from unless there are additional 
learnable sequential positional embeddings, i.e. number down the image patches.

% \begin{figure}[H]
% \begin{algorithm}[H]
% \KwData{A as Array to sort,}
% \KwResult{A as sorted Array} 

% int n $\leftarrow$ A.size \tcp*[l]{cache the initial size of A}


% \Repeat{n>1}{
% 	int newn $\leftarrow$ 1
% 	\For{int i=0; i<n-1; i++}
% 	{
% 		\If{A[i] > A[i+1]}
% 		{
% 			A.swap(i, +1);
% 		}
% 	}
% 	n  $\leftarrow$ newn
% }

% return A
% \caption{bubbleSort(Array A)}
% \end{algorithm}
%\caption[TBC]{Figure caption.v.}
% \end{figure}


\newpage
\section{"Towards End-to-End Image Compression with Transformers"}


% \begin{table}[H]
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% result XY & \multicolumn{6}{c|}{Rater A}\\\hline
%  \multirow{6}{*}{Rater B}& & \textbf{9} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{c(a) }\\\cline{2-7}
%  & \textbf{9}& 27	&0&	0	&0 & 0.5510204082\\\cline{2-7}
% &\textbf{0}	&0	&6	&0	&0&0.1224489796\\\cline{2-7}
% &\textbf{1}	&0	&1	&7	&0&0.1632653061\\\cline{2-7}
% &\textbf{2}	&0	&1	&7	&0&0.1632653061\\\cline{2-7}
% &\textbf{c(r)} & 0.5510204082 & 0.1632653061 & 0.2857142857 & 0& n=49\\\hline

% \end{tabular}
% \caption[Example data for mathematical equations]{A set of example data for further use in a mathematical equation.  }
% \end{center}
% \end{table}

\begin{center}
$Pr(e) = \displaystyle\sum_{i=1}^{m} c(r)_i \times c(a)_i $
\end{center}

\section{Image Generation}

Generative models.
GANs used to be the coolest thing until Transformers came along, but nonetheless 
still great. Typically, GANs pit 2 CNNs against each other, one called Generator to 
generate an image and one called Discriminator to determine how fake the image is 
compared to the input, relaying feedback to improve the Generator after each epoch. 
A GAN pits two Neural Networks (e.g. Convolutional) one against the other to generate 
new content (e.g. an image). The structure of a GAN is foundational to its design. 
As Fig.1 shows, the Generator CNN is shown on the left of the generated output. 
The Generator iteratively learns to create new synthetic data resembling real source data 
(e.g. regular or 360Â° images).  On the right of the generated output is the Discriminator CNN.
The Discriminator is trained to differentiate between real and synthetic data. 
In essence, the Discriminator gives the Generator feedback on its performance while 
simultaneously achieving incremental improvement in discernibility. 
Thus, each epoch the Generator produces more and more realistic data and the Discriminator iteratively improves its own ability to differentiate between real and synthetic data.


\section{"TransGAN"}
\subsection{"Two Pure Transformers Can Make One Strong GAN"}
NeurIPS 2021

Goal is to replace Generator and Discriminator in a GAN with Transformers free of 
convolutions. 

\section{First Principles of Neural Image Compression}

Deterministic, Probabilistic
GANs
JPEG/MPEG

\section{Commonly Used Metrics for Image Quality}
Metrics for the generated output image are: a BRISQUE score (reference-less) [5], 
Mean Squared Error (MSE) and Structural Similarity Index (SSIM). PSNR.
GANs use FID and Inception score. 
BRISQUE \citep{BRISQUE}
Divisive normalization, Gaussian properties from natural scene statistics
Process results in Gaussian unless it's distorted
BRISQUE too simple for user-generated content (UGC)