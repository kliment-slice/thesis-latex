\section{"An Image is Worth 16x16 Words"}

\textbf{Transformers for Image Recognition at Scale}


This seminal academic work was published in the International Conference on 
Learning Representations (ICLR) 2020 by a team from Google.
It presents the first Vision Transformer (ViT) for object detection
trained on the ImageNet dataset, a rather large natural images dataset common to research on 
image classification.


Since Chapter 1 already explained and formulated the ViT in detail, this section focuses
on outlining the outcomes from this publication. Refer to Figures 1.2 and 1.3 from Chapter 1, which
were originally developed for this paper.


The Self-Attention mechanism, detailed in section 1.2.2, is key to defining where the Transformer is able to 
find a certain object on an image. Notice the self-attention map plotted on top of the original images in 
Figure 2.1 below:

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{media/attention_map.png}
	\end{center}
	\caption[Attention Map Plot]{The attention map is plotted and superimposed on the 
	original images from the paper. \citep{dosovitskiy2020vit}}
\end{figure}


"An Image is Worth 16x16 Words" completely discards the notion of convolutions. 
The team used an image patch-based approach by breaking the image down to square blocks called patches.
These patches are used as embeddings to a Transformer to classify images.

Comparison between the position embeddings using cosine similarity points the transformer into
forming the object detection attention map. Figure 2.2 below shows a representation.


\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.3\textwidth]{media/embedviz.png}
	\end{center}
	\caption[Cosine Similarity of Image Patches]{The image patches are compared to each other using cosine similarity
	 to yield an indexed embedding map guiding the self-attention mechanism. \citep{dosovitskiy2020vit}}
\end{figure}

Typically, such object detection tasks in this field of research are dominated by Convolutional Neural Networks (CNNs).

However, the ViT model achieves a top-1 accuracy (matching highest class probability only) of 77.3\% on ImageNet, which 
compares to the accuracy of state-of-the-art CNNs.


According to the paper, the pre-trained ViT on the JFT-300M dataset beats all ResNets on all
datasets. Furthermore, it takes significantly less computational resources to train. \citep{dosovitskiy2020vit}

Compared to its convolutional counterpart (the ResNet), the ViT cost 75\% 
less resources to train and outperformed on ImageNet accuracy by 1\%. 
ViT uses approximately 2-4x less compute to attain the same performance 
(averaged over 5 test datasets). 

The total number of parameters is on the order of 100M.

Some of the results from the original publication are shown in Figure 2.3 below.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{media/vit_table.png}
	\end{center}
	\caption[Original ViT Results]{Results summary from the original ViT by Google. It shows 
    the ViT performance on popular classification benchmarks. \citep{dosovitskiy2020vit}}
	\end{figure}

% \begin{figure}[H]
% \begin{algorithm}[H]
% \KwData{A as Array to sort,}
% \KwResult{A as sorted Array} 

% int n $\leftarrow$ A.size \tcp*[l]{cache the initial size of A}


% \Repeat{n>1}{
% 	int newn $\leftarrow$ 1
% 	\For{int i=0; i<n-1; i++}
% 	{
% 		\If{A[i] > A[i+1]}
% 		{
% 			A.swap(i, +1);
% 		}
% 	}
% 	n  $\leftarrow$ newn
% }

% return A
% \caption{bubbleSort(Array A)}
% \end{algorithm}
%\caption[TBC]{Figure caption.v.}
% \end{figure}


\newpage
\section{"End-to-End Image Compression with Transformers"}

A recent publication in Association for the Advancement of Artificial Intelligence (AAAI) 2022 is
claiming to have achieved an end-to-end Transformer-based model for image compression and analysis.

"Towards End-to-End Image Compression and Analysis with Transformers" redesigns the ViT
to classify images from compressed features. \citep{Bai2022AAAI}

The research team replaces the patches and embeddings with a CNN-based lightweight encoder. 
The compressed latent space features from the encoder are fed into the Transformer.
The Transformer proceeds to classify the image without any regeneration or reconstruction. 
The framework includes a feature aggregation module, which blends the compressed and intermediate Transformer features. 
These features are then passed onto a Deconvolutional Neural Network 
and the image is regenerated. As a result, long-term dependencies from the Transformer self-attention mechanism 
are preserved.

Figure 2.4 below demonstrates the exact architecture used in the experiment.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{media/vit_compress_arch.png}
	\end{center}
	\caption[End-to-End Image Compression ViT]{The encoder and decoder architecture of the  
    End-to-End Image Compression ViT. \citep{Bai2022AAAI}}
	\end{figure}

\section{Overview of Image Generation with GANs}

Generative Adversarial Networks (GANs) have long been considered to be 
the most exciting innovation in Deep Learning. In a typical GAN architecture, 
two CNNs are competing against each other. One network is called the Generator, whose purpose is to 
generate an image. The other network is called the Discriminator, whose purpose is to determine 
how realistic the generated output by the Generator is. 

The Discriminator communicates feedback to the Generator after each epoch through a loss function.
The Discriminator is trained to minimize the loss, while the Generator is maximizing it.
The Generator iteratively learns to create new synthetic images resembling real source input image.
Then, the Discriminator is trained to differentiate between the real input data and generated synthetic data. 
The desired outcome is that a Generator would be able to fool the Discriminator that its synthetic output
is real. \citep{GANs} 


In essence, the Discriminator provides feedback to the Generator on its performance, while 
simultaneously achieving incremental improvements in its own discernibility. 

As a result, after each epoch, the Generator is expected to produce increasingly more realistic data,
according to a quantitative or qualitative (visual inspection) metric. In turn, the Discriminator becomes 
better at catching synthetic output, thereby having both networks compete against each other to 
refine the final generated image output.

\vspace{5mm}

Figure 2.4 below illustrates the architecture of a common GAN. Starting from random noise,
the Generator is iteratively trained to produce a quality generated image.
That image is then evaluated by the Discriminator whether it is real or synthetic.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{media/gan_arch.png}
	\end{center}
	\caption[GAN Architecture]{The architecture of a simple
    Generative Adversarial Network. \citep{GANs}}
	\end{figure}

Several common GAN applications include generating missing data (versus deterministic
interpolation), synthetic data, image-to-image translation, medical imaging, and art.


\section{First Principles of Neural Image Compression}

The explosion in popularity of Deep Learning methods as universal function approximators
is expected to revolutionize image compression. The key reason is that neural networks
could achieve orders of magnitude higher compression than lossy deterministic methodologies.
They can extract relevant information from the latent space of the image and are extremely efficient 
at packing it into a condensed version. The resulting compressed representations would require much less 
disk space or bandwidth for transmission.

Presently, efficient storage and transmission of image and video is still a developing and costly domain.
Thus, a Deep Learning approach to multimedia compression would, theoretically, enable higher compression ratios 
and an increase in visual quality. This approach would train a model to learn a compression function directly from data. 
Referred to as "Learned Multimedia Compression", this approach involves
computing a compressed representation of an input image. It uses separate models for both the encoder and decoder.
\citep{Principles}


Finally, a Deep Learning-based method could learn key steps of established deterministic compression algorithms.
This approach would serve as an improvement to the existing framework of popular compression techniques.
The most popular image compression technique is the Joint Photographic Expert Group format, JPEG, created in 1992. 
In short, it operates by using a Discrete Cosine Transform (DCT) to convert image information from the spatial to frequency domain. Then,
the compression algorithm discards high-frequency information, as it is not relevant to human visual perception. \citep{Principles}

Figure 2.6 illustrates JPEG compression at various thresholds, along with the compression ratios of each 
compressed image listed underneath.
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{media/jpeg_comp.png}
	\end{center}
	\caption[JPEG Compression]{A neural compression model would have to 
    outperform JPEG in compression ratio vs reconstructed image quality. \citep{GANs}}
	\end{figure}
An example of a Deep Learning-based compression model could be to improve the compression fidelity (output quality, compression ratio, 
and speed) of JPEG. To prevent information loss, a model can be trained to predict non-linearities 
in the compressed image related to blurring or noise. Furthermore, a model could learn image correction maps to correct distortions 
caused by JPEG compression. 
It can be trained to learn the latent feature space from example pairs of corrected JPEG compressed images and
their ground-truth originals. 


\section{Metrics for Image Quality}

Image Quality Assessment (IQA) is a methodology to evaluate distortions, aberrations, or
degradations in perceived image quality.
To evaluate the quality of a reconstructed image and benchmark the ViT-Score,
this thesis will use the following metrics:

\begin{itemize}
	\item Structural Similarity Index (SSIM)
	\item Mean Squared Error (MSE)
    \item Peak Signal-to-Noise Ratio (PSNR)
	\item Frechet Inception Distance (FID)
	\item Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)
\end{itemize}

\vspace{2mm}

\textbf{SSIM}


SSIM is a measure of the similarity between two images, on an interval $[-1, 1]$.

The higher SSIM value indicates a more similar image to the reference. 

It is based on the computing and combining three components: Luminance, Contrast, and Structure.
Luminance measures image brightness. Contrast measures how dark and bright pixels are distributed in an image. 
Structure captures edge definition. \citep{SSIM_bovik}
\vspace{2mm}

The exact mathematical formulation is:

\vspace{3mm}
\begin{center}
$SSIM(x,y) = \dfrac{(2\mu_x\mu_y + (k_1 L)^2) + (2 \sigma _{xy} + (k_2 L)^2)}{(\mu_x^2 + \mu_y^2+(k_1 L)^2) (\sigma_x^2 + \sigma_y^2+(k_2 L)^2)}$
\end{center}
\vspace{3mm}

Where, given a windows x and y of size $(N,N)$:

$\mu_x$ is the average of $x$'  $\mu_y$ is the average of $y$;  $\sigma _{x}^{2}$ the variance of $x$, $\sigma _{y}^{2}$ the variance of $y$;

$\sigma_{xy}$ the covariance of $x$ and $y$; $L$ is the pixel value dynamic range, i.e. $2^{\#bits\_per\_pixel}-1$;

and by default, given as constants are $k_{1}=0.01$ and $k_{2}=0.03$ \citep{SSIM_bovik}

\vspace{4mm}

\textbf{MSE}


MSE is a measure of the average squared error between an input and reference matrix (i.e. image). 
The lower the MSE value, the better the image quality.

The mathematical formulation is:

\vspace{3mm}

\begin{center}
$MSE=\dfrac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n (x_{ij}-y_{ij})^2 $
\end{center}


Where:
$m,n$ is the rows and columns of the input image,
$x_{ij}$ and $y_{ij}$ are pixel values from the input and generated image respectively. \citep{Metrics}


\vspace{4mm}

\textbf{PSNR}


PSNR is the ratio between the maximum possible signal intensity value and the corrupting noise present in the same signal. 
The higher the PSNR value, the better the image quality. \citep{Metrics}


The mathematical formulation becomes:
\vspace{3mm}

\begin{center}
$PSNR(x,y)=\dfrac{10\log_{10}[\max(\max(x),\max(y))]^2}{\vert{x-y}^2\vert}$
\end{center}

% \newpage

\textbf{FID}


Commonly used in analyzing GAN output quality, the Frechet Inception Distance (FID) compares two multidimensional Gaussian distributions. 

Given are:


$\mathcal{N}(\mu ,\Sigma )$, representing neural network features of the GAN generated image.

and 

$\mathcal{N}(\mu_w,\Sigma_w)$, representing the same features from the real images in a training dataset. 
\citep{FID}

\vspace{3mm}

Thus, the mathematical formulation is:


\begin{center}
$FID=||\mu -\mu _{w}||_{2}^{2} + tr(\Sigma +\Sigma _{w}-2(\Sigma ^{1/2}\Sigma _{w}\Sigma ^{1/2})^{1/2})$
\end{center}

\vspace{5mm}

\textbf{BRISQUE}


BRISQUE represents a referenceless image quality methodology where 0 is the perfect score and 100 worst.
This score could be interpreted as the tendency of an image to be photorealistic. The score is derived from
the Gaussian properties of natural scene statistics found in images.
Image quality is evaluated for the presence of corruption caused by blurs or graininess. 
An image with no distortions often has a score below 5. \citep{BRISQUE}

\vspace{3mm}

\NOTE{BRISQUE may be too simple to evaluate user-generated content, but is nonetheless useful in comparing
it to the ViT-Score.}
