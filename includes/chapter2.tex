\section{"An Image is Worth 16x16 Words"}

\subsection{Transformers for Image Recognition at Scale}

Now that we explained relevant transformer components, we can see how it applies to 2D signals, i.e. image matrices for classification purposes in image recognition.
The Vision Transformer or ViT, was very recently published for ICLR 2021 by a google team for a ImageNet trained transformer.
Tokenization happens at pixel level, so each pixel would have to attend to  each other pixel in the grid, which becomes too heavy to compute, on the order of $(250^2)^2$. 
To resolve that, the image is broken down into blocks of equal size, a 16x16 subset of the image called image patches. Then, unroll each image patch into a sequence (256x1), and index it with a positional embedding in a table. All of that is then fed into a standard Transformer, like from Attention is all you need. Finally, a feed forward classifier (MLP) makes the classification prediction, voila image recognition.
The total number of parameters is on the order of 100M.

"image is worth 16x16 words" completely discards the notion of convolutions. Compared to a convolutional counterpart (the ResNet), the ViT cost 75% less to train and beats accuracy on ImageNet by 1%. ViT uses approximately 2 − 4× less compute to attain the same performance (averaged over 5 datasets). Unlike variable size convolution kernels accross layers, the block size in the image transformer is able to pay attention within a single layer to anywhere on the image. 
CNNs have good inductive priors and can learn any function. However, this promotes locality (i.e. nearing pixels are probably most important).
One way to think of a ViT is a generalization of an MLP which itself is a generalization of a CNN. The ViT also learns very similarly to a CNN (e.g. filters with principal components).
Transformer, in a way, is a generalization of a feed forward network, but instead of fixed connections weights in an MLP, each connection weight (i.e. attention) is computed on the fly. That makes the Transformer, unlike the MLP, permutation invariant. In that it wouldn't know where information is coming from unless there are additional learnable sequential positional embeddings, i.e. number down the image patches.

% \begin{figure}[H]
% \begin{algorithm}[H]
% \KwData{A as Array to sort,}
% \KwResult{A as sorted Array} 

% int n $\leftarrow$ A.size \tcp*[l]{cache the initial size of A}


% \Repeat{n>1}{
% 	int newn $\leftarrow$ 1
% 	\For{int i=0; i<n-1; i++}
% 	{
% 		\If{A[i] > A[i+1]}
% 		{
% 			A.swap(i, +1);
% 		}
% 	}
% 	n  $\leftarrow$ newn
% }

% return A
% \caption{bubbleSort(Array A)}
% \end{algorithm}
%\caption[TBC]{Figure caption.v.}
% \end{figure}


\newpage
\section{"Towards End-to-End Image Compression with Transformers"}


% \begin{table}[H]
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% result XY & \multicolumn{6}{c|}{Rater A}\\\hline
%  \multirow{6}{*}{Rater B}& & \textbf{9} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{c(a) }\\\cline{2-7}
%  & \textbf{9}& 27	&0&	0	&0 & 0.5510204082\\\cline{2-7}
% &\textbf{0}	&0	&6	&0	&0&0.1224489796\\\cline{2-7}
% &\textbf{1}	&0	&1	&7	&0&0.1632653061\\\cline{2-7}
% &\textbf{2}	&0	&1	&7	&0&0.1632653061\\\cline{2-7}
% &\textbf{c(r)} & 0.5510204082 & 0.1632653061 & 0.2857142857 & 0& n=49\\\hline

% \end{tabular}
% \caption[Example data for mathematical equations]{A set of example data for further use in a mathematical equation.  }
% \end{center}
% \end{table}

\begin{center}
$Pr(e) = \displaystyle\sum_{i=1}^{m} c(r)_i \times c(a)_i $
\end{center}


\section{"TransGAN"}
\subsection{"Two Pure Transformers Can Make One Strong GAN"}

Sometimes you want to include text which include characters that could trigger commands. At this point it useful to wrap them into the \NOTE{verbatim} environment. The text is uninterpreted and listet as is.

You can also include existing code and highlight it's syntax, using the \NOTE{lstlisting} package. Look at the declarations.tex file for listings settings. The following example highlights xml syntax by given keywords.


\section{"First Principles of Deep Learning and Compression"}

Deterministic, Probabilistic
GANs
JPEG/MPEG

\section{Commonly Used Metrics for Image Quality}

SSIM, MSE, PSNR, BRISQUE