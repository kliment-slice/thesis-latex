\section{Results and Shortcomings}
We will review the merits from this work.

\subsection{Results}

\subsection{Potential Improvements to Approach}

\newpage
\section{Present and Future of Image Transformers}
Status quo of Transformers in Image Processing, Compression, Analysis, and Generation
Coveted Deep learning based Image compression 
In the deep learning/AI evolutionary process, still too early. Models have not been trained on enough image data.

GAN model only trained on finite set (ImageNet, CIFAR-10, Celeb-HQ faces etc) and resolution.
Need to train GAN on all images ever.


\section{Training and Cost Estimates}

REDO FOR ViT on all internet to cost 100M
A 512-core TPU v3 pod costs \$384/hr to use commercially on GCP. 
2.5k core-days means training the ViT cost 24hrs * \$384 * 5 of them = \$46k. 
That's for one of many ViT flavors.
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops

(like GPT-3 trained on all internet text, Vision T trained on all google images)

(\$100M+), GPT-3 cost \$10M-\$20M