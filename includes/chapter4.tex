\section{Results and Improvements}

\subsection{Results}

The most incomprehesible generated image "Tower", where the GAN failed to reconstruct a recognizable
object, expectedly has the lowest ViT-Score of all four images. This is a testament for the valuable 
insight the ViT-Score contributes.


PSNR, FID, and BRISQUE seem rather irrelevant due to their inability to capture synthetically induced
aberrations obvious to the human eye.

"Logo" metrics could be even higher, if the background were to be removed (like the input).

\subsection{Potential Improvements to Architecture}

The ViT-Score definition could be further improved or experimented with.
For the purpose of this project, the ViT-Score was based on how many of the top100 labels match
between the input and generated images.

The ViT-Score can take into account probability of each label found (included in script output).
While the label probability is stable when working with corrupted images, it is rather 
unstable when working with generated images.

Features are typically found to be in the deeper layers of the GAN network. 
The latent space is hard to decipher, but a Transformer is efficient in packing information from it 
to a 1-dimensional vector.

Evaluating output quality from Generative Adversarial Networks (GANs) is 
still a developing field using non-Deep Learning-adapted assessment methods. 
For the purpose of this thesis, a GAN was used as a placeholder 
for a future, coveted, and highly desirable Deep Learning-based image compression mechanism.

Need a GAN trained on all images, not just popular datasets such as ImageNet or Celeb. 
Perhaps the next generation ViT needs to be trained on all internet images, or at least a sizable, diverse, and
representative subset.

The natural performance limit to the capacity of this model comes from its training sets.

Unique positional encoding can also be achieved using trigonometric representation.
For example, a full sentence from text or perhaps a row of pixels from an image could be represented by the various
periods of a sinusoid. Thus, the exact location of each token would be unique.

\section{Optimization} 

Experiment and change loss functions (MSE was used, can use a GAN specific loss like FID)
Learning rate reduction on plateau was used.
Most valuable technique, which contributed to the GAN improving its quality of generated image was
reducing the learning\_rate as the model trains.

Stochastic Gradient Descent (SGD) was used as an optimizer for the GAN.

Perhaps trying Adam could achieve better results on certain types of images, though probably
not on average.

Furthermore, experimenting with input image types to cater to what the generative model is best trained on
could yield much better results.

Other options to endlessly experiment with include introducing regularization during training such as residual dropout and label smoothing.

Analyze latent space vector with transformer model (not a ViT, but a transformer adaptation)
Steer the GAN faster into training to compress
Slow computational times


%\newpage

\section{Present and Future of Image Transformers}

\subsection{Status Quo}
Increase in relevant recent publications (13 alone in 2022 thus far) supports the vision of tying 
vision transformers to image compression.

Status quo of Transformers in Image Processing, Compression, Analysis, and Generation
Coveted Deep learning based Image compression 
In the deep learning/AI evolutionary process, still too early. Models have not been trained on enough image data.

\subsection{Future Developments}
GAN model only trained on finite set (ImageNet, CIFAR-10, Celeb-HQ faces etc) and resolution.
Need to train GAN on all images ever.

"TransGAN: Two Pure Transformers Can Make One Strong GAN" \citep{jiang2021transgan}
NeurIPS 2021

Goal is to replace Generator and Discriminator in a GAN with Transformers free of 
convolutions. 
Deterministic, Probabilistic
GANs
JPEG/MPEG

\subsection{Training and Costs}

Need a ViT on all internet to cost 100M
A 512-core TPU v3 pod costs \$384/hr to use commercially on GCP. 
2.5k core-days means training the ViT cost 24hrs * \$384 * 5 of them = \$46k. 
That's for one of many ViT flavors.
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops

(like GPT-3 trained on all internet text, Vision T trained on all google images)

(\$100M+), GPT-3 cost \$10M-\$20M
r. In terms of image reconstruction on the decoder side, our image reconstructor needs more
FLOPs than mbt-m due to the feature aggregation module. In
terms of image classification on the decoder side, our image
classifier directly performs inference from the compressed
features without the image reconstruction process, and thus
needs far less computational cost compared with the inference from reconstructed RGB images.
