\section{Results and Optimization}
We will review the merits from this work.

\subsection{Results}

Features are in deep layers of the GAN network. 
Latent space is hard to decipher
ViT score definition: how many of the top100 labels match
Can take into account probability of label (included in ViT).
While label probability is stable when working with corrupted images (demo), 
unstable when working with generated images.

\subsection{Potential Improvements to Architecture}

Analyze latent space vector with transformer model (not a ViT, but a transformer adaptation)
Steer the GAN faster into training to compress
Slow computational times
Need a GAN trained on all images, not just ImageNet or Celeb 
Need ViT trained on all images
Natural limit to capacity of this model comes from training sets.

\subsection{Optimization}

Experiment and change loss functions (MSE was used, can use a GAN specific loss like FID)

Most valuable technique:
(reducing learning\_rate as the model trains)
changing input images to cater to what the generative model is trained on.
SGD optimizer for GAN

A lot of options still not figured out.
regularization during training: residual dropout, label smoothing

\newpage
\section{Present and Future of Image Transformers}
Status quo of Transformers in Image Processing, Compression, Analysis, and Generation
Coveted Deep learning based Image compression 
In the deep learning/AI evolutionary process, still too early. Models have not been trained on enough image data.

GAN model only trained on finite set (ImageNet, CIFAR-10, Celeb-HQ faces etc) and resolution.
Need to train GAN on all images ever.

"TransGAN: Two Pure Transformers Can Make One Strong GAN" \citep{jiang2021transgan}
NeurIPS 2021

Goal is to replace Generator and Discriminator in a GAN with Transformers free of 
convolutions. 


\section{Training and Cost Estimates}

Need a ViT on all internet to cost 100M
A 512-core TPU v3 pod costs \$384/hr to use commercially on GCP. 
2.5k core-days means training the ViT cost 24hrs * \$384 * 5 of them = \$46k. 
That's for one of many ViT flavors.
To train a ViT on the whole TACC Frontera at 20k teraflops (top10) or 
Stampede at 10k tflops (top25), it would take respectively about a minute and 2 minutes.
tpu v3 is 420 teraflops * 2500 = 1M Tflops

(like GPT-3 trained on all internet text, Vision T trained on all google images)

(\$100M+), GPT-3 cost \$10M-\$20M