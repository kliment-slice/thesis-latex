# Master's Thesis Report LaTeX Document
## The University of Texas at Austin, Oden Institute for Computational Science and Engineering

This repository contains the LaTeX code to generate the official PDF document for
"Investigation of Transformer-based Methods for Image Compression, Analysis, and Generation"

Report
=============================================================
The report includes a summary of my research on and experimentation with vision transformers
and transformer-based vision models (i.e. architecture that includes the attention mechanism) with respect to digital images.
The original contribution of this project lies in exploring the novel domain of the highly generalizable and overparameterized
Vision Transformer models with regards to image processing.

### Contents

#### 1. Introduction to Vision Transformers
##### 1.1. Motivation (generalization, overparameterization)
##### 1.2. Brief History (language models)
##### 1.3. Principles of Operation (diagrams) 
##### 1.4. Formulation (math)
##### 1.5. Implementations (pre-trained models)
##### 1.6 Computational constraints
##### 1.7 Technologies used (GPU training, TACC)

#### 2. Literature Review: Vision Transformers and Neural Image Compression 
##### 2.1. Image is worth 16x16
##### 2.2. Transformer-based Image Compression (Ming Lu)
##### 2.3. Towards End-to-End Image Compression and Analysis with Transformers (Yuanchao Bai)
##### 2.4. TransGANs

#### 3. Image Compression with Transformers
##### 3.1. Generative image compression and generation overview (vineeth)
###### 3.1.1 Architecture
###### 3.1.2 Latent space vector representation after compression
##### 3.2. Outputs from implementation, graphs, tables
##### 3.3. Visual Inspection (natural performance asymptote)
##### 3.4. GAN Quantitative (FID score, inception score)
##### 3.5. Difference vs original, SSIM, MSE, PSNR, BRISQUE (and definitions of each)
##### 3.6. Optimization Techniques (reducing learning_rate as the model trains)

#### 4. Future of Vision Transformers, Compression, and Generation
##### 4.1. Deep learning based Image compression 
##### 4.2. Training (like GPT-3 trained on all internet text, Vision T trained on all google images)
##### 4.3. Cost ($100M+), GPT-3 cost $10M-$20M

#### 5. Closing remarks
##### 5.1. Key contributions (experimentation with Vision Transformers, nascent field)
##### 5.2. Acknowledgments (LIVE lab, TACC)

#### 6. Bibliography